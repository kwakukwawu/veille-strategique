# üìö GUIDE PRATIQUE - Veille Strat√©gique √âtape par √âtape

## Bienvenue! Vous √™tes novice? Voici comment fonctionnent les fichiers ensemble.

---

## üß† COMPRENDRE L'ARCHITECTURE

### **Le flux global** (Simplifi√©)

```
1. Les SCRAPERS cherchent les offres sur Internet
   ‚Üì
2. Les offres sont SAUVEGARD√âES en base de donn√©es
   ‚Üì
3. L'API expose les offres via HTTP
   ‚Üì
4. L'INTERFACE WEB affiche tout joliment
```

### **Les 3 couches de code**

```
FRONTEND (Ce que l'utilisateur voit)
‚îú‚îÄ‚îÄ HTML/CSS/JavaScript
‚îî‚îÄ‚îÄ Affiche les offres, formulaires, etc.

    ‚Üï (Communication par API)

API (La passerelle)
‚îú‚îÄ‚îÄ Routes.py = Les endpoints
‚îú‚îÄ‚îÄ Auth.py = V√©rifier qui tu es
‚îî‚îÄ‚îÄ Middleware.py = V√©rifier les permissions

    ‚Üï (Requ√™tes SQL)

BACKEND (Le cerveau)
‚îú‚îÄ‚îÄ Models.py = Structure des donn√©es
‚îú‚îÄ‚îÄ Scrapers = Cherchent les offres
‚îú‚îÄ‚îÄ Scheduler = Lance les scrapers √† l'heure
‚îî‚îÄ‚îÄ Database = SQLite (stockage)
```

---

## üöÄ D√âMARRER LOCALEMENT (Pas √† Pas)

### **√âtape 1: Pr√©parer l'environnement**

```powershell
# Ouvrir PowerShell dans le dossier du projet
cd c:\Users\HP\Desktop\veille-strategique

# Cr√©er l'environnement virtuel
python -m venv venv

# L'activer (tu verras (venv) avant le prompt)
.\venv\Scripts\activate
```

### **√âtape 2: Installer les paquets**

```powershell
# Aller dans le dossier backend
cd backend

# Installer tout ce qui est dans requirements.txt
pip install -r requirements.txt

# Attendre que tout s'installe... (1-2 minutes)
```

### **√âtape 3: D√©marrer l'app**

```powershell
# Toujours dans backend/
python app.py
```

Tu devrais voir:
```
‚úì Base de donn√©es initialis√©e
‚úì Donn√©es par d√©faut charg√©es
‚úì Application Flask cr√©√©e et configur√©e
‚úì Planificateur de scraping d√©marr√©

 * Running on http://localhost:5000
```

‚úÖ **Succ√®s!** Ouvre **http://localhost:5000** dans ton navigateur.

---

## üìÇ O√ô FAIRE QUOI?

### **Je veux ajouter une nouvelle SOURCE de scraping**

**Exemple**: Scraper ENABEL (Agence belge)

**Fichiers √† cr√©er/modifier**:

1. **Cr√©er** `backend/scraping/scrapers/enabel_scraper.py`

```python
from .base_scraper import BaseScraper

class ENABELScraper(BaseScraper):
    def __init__(self):
        super().__init__('ENABEL')
        self.base_url = 'https://enabel.be/tenders'
    
    def scrape(self, mots_cles=None):
        """Scraper les appels d'ENABEL"""
        offres = []
        
        # R√©cup√©rer la page
        soup = self.recuperer_page(self.base_url)
        if not soup:
            return offres
        
        # Chercher les √©l√©ments HTML contenant les offres
        articles = soup.find_all('div', class_='tender')
        
        for article in articles:
            titre = self.extraire_texte(article.find('h3'))
            lien = article.find('a', href=True)
            url_offre = lien['href'] if lien else None
            
            if titre and url_offre:
                offre = self.creer_offre(
                    titre=titre,
                    source=self.source_nom,
                    url=url_offre,
                    partenaire='ENABEL',
                    type_offre='Appel d\'offres'
                )
                offres.append(offre)
        
        return self.nettoyer_offres_doublons(offres)
```

2. **Enregistrer** dans `backend/scraping/scrapers/__init__.py`:

```python
from .enabel_scraper import ENABELScraper
__all__ = [..., 'ENABELScraper']
```

3. **Ajouter au Scheduler** dans `backend/scraping/scheduler.py`:

```python
self.scrapers = {
    'giz': GIZScraper(),
    'un': UNScraper(),
    'educarriere': EduCarriereScraper(),
    'enabel': ENABELScraper(),  # ‚Üê NOUVEAU
}

# Ajouter le job
self.scheduler.add_job(
    func=self._executer_scraper,
    args=['enabel'],
    trigger=CronTrigger(hour=12, minute=0),
    id='scraper_enabel',
    name='Scraper ENABEL',
    replace_existing=True
)
```

4. **Tester imm√©diatement** (sans attendre 12h):

```powershell
# Dans Python interactif
from app import create_app
from scraping.scheduler import scheduler

app = create_app()
scheduler.executer_maintenant('enabel')
```

---

### **Je veux ajouter une nouvelle PAGE web**

**Exemple**: Page "√Ä propos"

**Fichiers √† cr√©er/modifier**:

1. **Cr√©er** `frontend/templates/apropos.html`:

```html
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <title>√Ä propos - Veille Strat√©gique</title>
    <link rel="stylesheet" href="/static/css/style.css">
</head>
<body>
    <header>
        <div class="header-container">
            <div class="logo">
                <div class="logo-icon">VS</div>
                <span>Veille Strat√©gique</span>
            </div>
            <nav>
                <a href="/">Accueil</a>
                <a href="/offres">Offres</a>
                <a href="/apropos">√Ä propos</a>
            </nav>
        </div>
    </header>

    <main>
        <div class="container">
            <h1>√Ä propos de Veille Strat√©gique</h1>
            <p>Nous sommes une plateforme d√©di√©e...</p>
        </div>
    </main>

    <footer>
        <p>&copy; 2026 Veille Strat√©gique</p>
    </footer>
</body>
</html>
```

2. **Ajouter la route** dans `backend/api/frontend_routes.py`:

```python
@frontend_bp.route('/apropos')
def apropos():
    return render_template('apropos.html')
```

3. **Rafra√Æchir le navigateur** - C'est pr√™t!

---

### **Je veux ajouter un nouvel ENDPOINT API**

**Exemple**: Endpoint pour obtenir les offres "urgentes"

**Fichier √† modifier**: `backend/api/routes.py`

```python
@api_bp.route('/offres/urgentes', methods=['GET'])
def offres_urgentes():
    """Obtenir les offres urgentes (qui ferment dans 7 jours)"""
    from datetime import datetime, timedelta
    
    date_limite = datetime.utcnow() + timedelta(days=7)
    
    offres = Offre.query.filter(
        Offre.date_cloturation <= date_limite,
        Offre.actif == True
    ).all()
    
    return jsonify({
        'total': len(offres),
        'offres': [o.to_dict() for o in offres]
    }), 200
```

**Tester dans le navigateur**:
```
http://localhost:5000/api/offres/urgentes
```

---

### **Je veux modifier la BASE DE DONN√âES**

**Exemple**: Ajouter un champ "Budget" aux offres

1. **Modifier le mod√®le** dans `backend/database/models.py`:

```python
class Offre(db.Model):
    # ... (fields existants)
    budget = db.Column(db.Float)  # ‚Üê NOUVEAU
    devise = db.Column(db.String(10), default='USD')  # ‚Üê NOUVEAU
```

2. **Cr√©er une migration** (√† faire plus tard):

```powershell
# Pour l'instant, la base se recr√©e automatiquement en dev
# En production, il faudrait:
# alembic revision --autogenerate -m "Ajouter budget aux offres"
# alembic upgrade head
```

3. **Red√©marrer l'app**:

```powershell
# Ctrl+C pour arr√™ter
# Puis relancer:
python app.py
```

---

### **Je veux ajouter un MOT-CL√â**

**Option 1: Directement en base (via API)**

```powershell
# Dans Python interactif
from app import create_app
from scraping.keyword_manager import KeywordManager

app = create_app()
with app.app_context():
    KeywordManager.ajouter_mot_cle('√ânergie renouvelable', 'Cha√Æne de valeur')
    print("‚úì Mot-cl√© ajout√©")
```

**Option 2: Via l'API HTTP**

```bash
curl -X POST http://localhost:5000/api/mots-cles \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer demo-token-admin@veille.ci" \
  -d '{"mot":"Aquaculture","categorie":"Cha√Æne de valeur"}'
```

---

### **Je veux changer les COULEURS du logo**

**Fichier**: `frontend/static/css/style.css` (lignes 1-20)

```css
:root {
    --primary-orange: #FF8C00;      /* Change ici */
    --primary-blue: #0066CC;        /* Change ici */
    --dark-black: #1a1a1a;          /* Change ici */
}
```

Les couleurs actuelles:
- **Orange**: `#FF8C00`
- **Bleu**: `#0066CC`
- **Noir**: `#1a1a1a`

Rafra√Æchis le navigateur, et c'est appliqu√© partout!

---

## üîç COMPRENDRE UN SCRAPER

### **Anatomie d'un Scraper**

```python
class MonScraper(BaseScraper):
    def __init__(self):
        # Initialiser avec le nom de la source
        super().__init__('Mon Source')
        self.base_url = 'https://example.com'
    
    def scrape(self, mots_cles=None):
        # mots_cles = ['Agriculture', 'Cacao', ...]
        
        offres = []
        
        # R√©cup√©rer la page HTML
        soup = self.recuperer_page(self.base_url)
        if not soup:
            return offres  # Erreur r√©seau
        
        # Trouver les √©l√©ments
        articles = soup.find_all('div', class_='article')
        
        for article in articles:
            # Extraire les infos
            titre = self.extraire_texte(article.find('h2'))
            lien = article.find('a', href=True)['href']
            
            # V√©rifier si contient les mots-cl√©s
            mots_trouves = self.matcher_mots_cles(titre, mots_cles)
            
            # Cr√©er l'offre
            offre = self.creer_offre(
                titre=titre,
                source=self.source_nom,
                url=lien,
                mots_cles_trouves=mots_trouves
            )
            offres.append(offre)
        
        return offres
```

### **M√©thodes utiles (de BaseScraper)**

| M√©thode | Utilit√© | Exemple |
|---------|---------|---------|
| `recuperer_page(url)` | T√©l√©charger + parser HTML | `soup = self.recuperer_page('https://...')` |
| `extraire_texte(element)` | Lire le texte d'un √©l√©ment HTML | `titre = self.extraire_texte(soup.find('h2'))` |
| `matcher_mots_cles(texte, mots)` | Trouver les mots-cl√©s | `mots_trouves = self.matcher_mots_cles(titre, ['Cacao'])` |
| `creer_offre(...)` | Cr√©er un dict standardis√© | `offre = self.creer_offre(titre='...', source='...')` |
| `nettoyer_offres_doublons(offres)` | Supprimer les doublons | `offres = self.nettoyer_offres_doublons(offres)` |

---

## üêõ D√âPANNAGE

### **Probl√®me: "ModuleNotFoundError: No module named 'flask'"**

```powershell
# Tu as oubli√© d'installer les d√©pendances!
pip install -r backend/requirements.txt
```

### **Probl√®me: "Address already in use"**

```powershell
# Un autre processus utilise le port 5000
# Soit attendre 2 minutes, soit:

# Trouver le processus
netstat -ano | findstr :5000

# Tuer le processus (remplacer PID)
taskkill /PID 1234 /F
```

### **Probl√®me: Base de donn√©es corrompue**

```powershell
# Supprimer la base et la recr√©er
rm backend\veille_strategique.db

# Relancer app.py
python app.py
```

### **Probl√®me: Les mots-cl√©s ne s'affichent pas**

```powershell
# V√©rifier qu'ils sont en base
python

from app import create_app
from database.models import MotsCles

app = create_app()
with app.app_context():
    mots = MotsCles.query.all()
    for m in mots:
        print(m.mot)
```

---

## üìä V√âRIFIER QUE TOUT FONCTIONNE

```powershell
# 1. V√©rifier la base de donn√©es
python
>>> from app import create_app
>>> from database.models import Offre, MotsCles
>>> app = create_app()
>>> with app.app_context():
...     print(f"Offres: {Offre.query.count()}")
...     print(f"Mots-cl√©s: {MotsCles.query.count()}")

# 2. Tester un scraper
>>> with app.app_context():
...     from scraping.scrapers import GIZScraper
...     scraper = GIZScraper()
...     offres = scraper.scrape()
...     print(f"Trouv√©: {len(offres)} offres")

# 3. Tester l'API
# Ouvre dans le navigateur:
# http://localhost:5000/api/stats
```

---

## üéØ R√âSUM√â DES FICHIERS CL√âS

| Fichier | R√¥le |
|---------|------|
| `app.py` | Lance tout |
| `config.py` | Param√®tres globaux |
| `database/models.py` | Structure des donn√©es |
| `database/database.py` | Initialise la BD |
| `scraping/scrapers/` | Cherchent les offres |
| `scraping/scheduler.py` | Lance les scrapers √† l'heure |
| `scraping/keyword_manager.py` | G√®re les mots-cl√©s |
| `api/routes.py` | Les endpoints HTTP |
| `api/auth.py` | Connexion/auth |
| `api/middleware.py` | Permissions |
| `frontend/templates/` | Pages HTML |
| `frontend/static/css/style.css` | Couleurs & design |
| `frontend/static/js/app.js` | Logique client (fetch, etc.) |

---

## ‚úÖ CHECKLIST - Avant de lancer

- [ ] Python 3.8+ install√©
- [ ] Virtual env activ√© (`venv\Scripts\activate`)
- [ ] D√©pendances install√©es (`pip install -r requirements.txt`)
- [ ] Tu es dans le dossier `backend/`
- [ ] Pas d'erreur au lancement (`python app.py`)
- [ ] Page d'accueil s'ouvre en `http://localhost:5000`
- [ ] API r√©pond: `http://localhost:5000/api/stats`

---

## üöÄ PROCHAINES √âTAPES

1. **Tester les scraping**: Ajoute tes propres sources
2. **Personnaliser les mots-cl√©s**: Ajoute ceux sp√©cifiques √† ton secteur
3. **Am√©liorer le design**: Change les couleurs, ajoute un logo
4. **Ajouter des utilisateurs**: Cr√©e des comptes dans la base
5. **Mettre en production**: Deploy sur un serveur (Heroku, PythonAnywhere, etc.)

---

**Besoin d'aide? Regarde les fichiers avec üí≠ Commentaires - ils expliquent tout!**

Bonne chance! üéâ
